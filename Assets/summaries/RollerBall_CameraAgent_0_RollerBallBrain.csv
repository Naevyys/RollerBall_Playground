Steps,Policy/Entropy,Policy/Learning Rate,Environment/Cumulative Reward,Environment/Episode Length,Policy/Extrinsic Value Estimate,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss
10000,1.4082272,0.00029700945,-0.3371104815864023,27.274787535410766,-0.4550515,-0.3352272727272727,0.19651383,0.16978535
20000,1.3940912,0.00029101231,-0.2903225806451613,39.07258064516129,-0.37299085,-0.2903225806451613,0.14672829,0.17589214
30000,1.3752391,0.0002850079,-0.3413654618473896,38.77510040160642,-0.37688673,-0.344,0.15378119,0.17459963
40000,1.3502811,0.00027900728,-0.4476987447698745,40.79916317991632,-0.4717534,-0.44537815126050423,0.116943166,0.17179936
50000,1.3347363,0.00027300508,-0.2745762711864407,32.8,-0.4212294,-0.2745762711864407,0.15630667,0.16773653
60000,1.3201187,0.00026700323,-0.40192926045016075,31.05144694533762,-0.49707595,-0.40836012861736337,0.1429689,0.16888309
70000,1.3104619,0.0002610001,-0.38461538461538464,28.505917159763314,-0.49339664,-0.378698224852071,0.16933165,0.16762671
80000,1.3044101,0.00025500884,-0.3568904593639576,34.15547703180212,-0.47535887,-0.3568904593639576,0.14184844,0.16822389
90000,1.3027943,0.00024901098,-0.2482758620689655,33.33793103448276,-0.39310122,-0.2482758620689655,0.1601044,0.17458452
