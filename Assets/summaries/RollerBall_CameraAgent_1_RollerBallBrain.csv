Steps,Policy/Entropy,Policy/Learning Rate,Environment/Cumulative Reward,Environment/Episode Length,Policy/Extrinsic Value Estimate,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss
10000,1.4172958,0.00029701463,-2.1942856748189246,174.25,-0.21693297,-2.1942855066486766,0.022920866,0.096187934
20000,1.4179648,0.00029101627,-2.4484443991548486,199.86666666666667,-0.6390067,-2.448444212807549,0.0081638545,0.10116018
30000,1.4140459,0.0002850144,-1.974499943573028,249.025,-0.5530166,-1.9744997143745422,0.017657863,0.09754397
40000,1.4088606,0.0002790075,-2.5309676644542525,341.64516129032256,-0.51369673,-2.615999589363734,0.007656604,0.09827652
50000,1.4077556,0.00027299864,-0.11409088418903676,110.13636363636364,-0.023671152,-0.1125841485650352,0.025314424,0.097099684
60000,1.4062886,0.00026698897,0.14470087381828034,84.62393162393163,0.13439494,0.1412069910559161,0.030218467,0.10375222
70000,1.4017808,0.000261002,0.35427633022252275,63.98026315789474,0.38482603,0.35348691359946605,0.02275584,0.09495855
