Steps,Policy/Entropy,Policy/Learning Rate,Environment/Cumulative Reward,Environment/Episode Length,Policy/Extrinsic Value Estimate,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss
10000,1.4105685,0.00029701172,-0.32951289398280803,27.595988538681947,-0.461692,-0.3275862068965517,0.14785181,0.19042288
20000,1.3957119,0.00029100326,-0.3851851851851852,35.48148148148148,-0.47318494,-0.3874538745387454,0.120174706,0.19164488
30000,1.3772813,0.000284995,-0.3054662379421222,31.228295819935692,-0.46191958,-0.3054662379421222,0.13657734,0.19219768
40000,1.3649513,0.00027899956,-0.4,34.864285714285714,-0.4774787,-0.3978494623655914,0.12617801,0.19354576
50000,1.3526207,0.00027299105,-0.250996015936255,38.18326693227092,-0.40698755,-0.25396825396825395,0.12543455,0.18652022
60000,1.3487875,0.00026699158,-0.09774436090225563,36.28195488721804,-0.25443628,-0.09774436090225563,0.13810833,0.19758749
